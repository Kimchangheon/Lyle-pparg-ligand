{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem, DataStructs\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load the dataset\n",
    "dataframe = pandas.read_csv(\"F:/UOSEST/Data/Lyle-pparg-ligand/input/pubchem_data/processed/pparg_ligand_data.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "not_ligand    762\n",
       "ligand         34\n",
       "Name: Type, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "d = dataframe\n",
    "sns.countplot(d['Type'])\n",
    "d['Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function binary_focal_loss_fixed at 0x0000022AE820A8C8>\n",
      "<function categorical_focal_loss_fixed at 0x0000022AEFAAB0D0>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Define our custom loss function.\n",
    "\"\"\"\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "import dill\n",
    "\n",
    "\n",
    "def binary_focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Binary form of focal loss.\n",
    "      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
    "      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
    "    References:\n",
    "        https://arxiv.org/pdf/1708.02002.pdf\n",
    "    Usage:\n",
    "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n",
    "               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "\n",
    "    return binary_focal_loss_fixed\n",
    "\n",
    "\n",
    "def categorical_focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Softmax version of focal loss.\n",
    "           m\n",
    "      FL = âˆ‘  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)\n",
    "          c=1\n",
    "      where m = number of classes, c = class and o = observation\n",
    "    Parameters:\n",
    "      alpha -- the same as weighing factor in balanced cross entropy\n",
    "      gamma -- focusing parameter for modulating factor (1-p)\n",
    "    Default value:\n",
    "      gamma -- 2.0 as mentioned in the paper\n",
    "      alpha -- 0.25 as mentioned in the paper\n",
    "    References:\n",
    "        Official paper: https://arxiv.org/pdf/1708.02002.pdf\n",
    "        https://www.tensorflow.org/api_docs/python/tf/keras/backend/categorical_crossentropy\n",
    "    Usage:\n",
    "     model.compile(loss=[categorical_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "    def categorical_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred: A tensor resulting from a softmax\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        # Scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "\n",
    "        # Clip the prediction value to prevent NaN's and Inf's\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "        # Calculate Cross Entropy\n",
    "        cross_entropy = -y_true * K.log(y_pred)\n",
    "\n",
    "        # Calculate Focal Loss\n",
    "        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n",
    "\n",
    "        # Sum the losses in mini_batch\n",
    "        return K.sum(loss, axis=1)\n",
    "\n",
    "    return categorical_focal_loss_fixed\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Test serialization of nested functions\n",
    "    bin_inner = dill.loads(dill.dumps(binary_focal_loss(gamma=2., alpha=.25)))\n",
    "    print(bin_inner)\n",
    "\n",
    "    cat_inner = dill.loads(dill.dumps(categorical_focal_loss(gamma=2., alpha=.25)))\n",
    "    print(cat_inner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mols = []\n",
    "fps = []\n",
    "\n",
    "#get molecules and then get fingerprints from those\n",
    "for index, row in dataframe.iterrows():\n",
    "    mol = Chem.MolFromSmiles(row['SMILES'])\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2)\n",
    "    mols.append(mol)\n",
    "    fps.append(fp)\n",
    "\n",
    "#Convert the RDKit vectors into numpy arrays\n",
    "#Based on: http://www.rdkit.org/docs/Cookbook.html#using-scikit-learn-with-rdkit\n",
    "np_fps = []\n",
    "for fp in fps:\n",
    "    arr = numpy.zeros((1,))\n",
    "    DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "    np_fps.append(arr)\n",
    "\n",
    "np_fps_array = numpy.array(np_fps)\n",
    "\n",
    "#Need to encode my classes\n",
    "#Ligand = 0, not_ligand = 1\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(dataframe['Type'])\n",
    "enc_y = encoder.transform(dataframe['Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import losses\n",
    "custom_object = {'binary_focal_loss_fixed': dill.loads(dill.dumps(binary_focal_loss(gamma=2., alpha=.25))),\n",
    "                 'categorical_focal_loss_fixed': dill.loads(dill.dumps(categorical_focal_loss(gamma=2., alpha=.25))),\n",
    "                 'categorical_focal_loss': categorical_focal_loss,\n",
    "                 'binary_focal_loss': binary_focal_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the machine learning model\n",
    "from keras import optimizers\n",
    "def create_deep_learning_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(2048, input_dim=2048, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], optimizer='sgd', metrics=['accuracy'])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "tmp = [[x,y] for x, y in zip(np_fps_array, enc_y)]\n",
    "tmp = shuffle(tmp)\n",
    "X = [n[0] for n in tmp]\n",
    "Y = [n[1] for n in tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= np.array(X)\n",
    "Y= np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(X)\n",
    "p = 0.8\n",
    "X_test = X[int(n*p):]\n",
    "Y_test = Y[int(n*p):]\n",
    "\n",
    "X_train = X[:int(n*p)]\n",
    "Y_train = Y[:int(n*p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    608\n",
       "0     28\n",
       "dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(Y_train)\n",
    "pandas.Series(Y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    154\n",
       "0      6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(Y_test)\n",
    "pandas.Series(Y_test).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=12, ratio = 'minority')\n",
    "X_train, Y_train = sm.fit_sample(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/220\n",
      "1216/1216 [==============================] - 2s 1ms/step - loss: 0.1413 - acc: 0.5263\n",
      "Epoch 2/220\n",
      "1216/1216 [==============================] - 1s 491us/step - loss: 0.0714 - acc: 0.5000\n",
      "Epoch 3/220\n",
      "1216/1216 [==============================] - 1s 503us/step - loss: 0.0397 - acc: 0.5000\n",
      "Epoch 4/220\n",
      "1216/1216 [==============================] - 1s 492us/step - loss: 0.0307 - acc: 0.5000\n",
      "Epoch 5/220\n",
      "1216/1216 [==============================] - 1s 472us/step - loss: 0.0278 - acc: 0.5000\n",
      "Epoch 6/220\n",
      "1216/1216 [==============================] - 1s 505us/step - loss: 0.0267 - acc: 0.5000\n",
      "Epoch 7/220\n",
      "1216/1216 [==============================] - 1s 499us/step - loss: 0.0261 - acc: 0.5000\n",
      "Epoch 8/220\n",
      "1216/1216 [==============================] - ETA: 0s - loss: 0.0257 - acc: 0.500 - 1s 507us/step - loss: 0.0258 - acc: 0.5000\n",
      "Epoch 9/220\n",
      "1216/1216 [==============================] - 1s 536us/step - loss: 0.0256 - acc: 0.5000\n",
      "Epoch 10/220\n",
      "1216/1216 [==============================] - 1s 501us/step - loss: 0.0253 - acc: 0.5000\n",
      "Epoch 11/220\n",
      "1216/1216 [==============================] - 1s 494us/step - loss: 0.0251 - acc: 0.5000\n",
      "Epoch 12/220\n",
      "1216/1216 [==============================] - 1s 499us/step - loss: 0.0249 - acc: 0.5000\n",
      "Epoch 13/220\n",
      "1216/1216 [==============================] - 1s 492us/step - loss: 0.0248 - acc: 0.5000\n",
      "Epoch 14/220\n",
      "1216/1216 [==============================] - 1s 494us/step - loss: 0.0246 - acc: 0.5000\n",
      "Epoch 15/220\n",
      "1216/1216 [==============================] - 1s 499us/step - loss: 0.0244 - acc: 0.5000\n",
      "Epoch 16/220\n",
      "1216/1216 [==============================] - 1s 500us/step - loss: 0.0242 - acc: 0.5000\n",
      "Epoch 17/220\n",
      "1216/1216 [==============================] - 1s 493us/step - loss: 0.0240 - acc: 0.5000\n",
      "Epoch 18/220\n",
      "1216/1216 [==============================] - ETA: 0s - loss: 0.0238 - acc: 0.5009- ETA: 0s - loss: 0.0237 - ac - 1s 513us/step - loss: 0.0238 - acc: 0.5000\n",
      "Epoch 19/220\n",
      "1216/1216 [==============================] - 1s 500us/step - loss: 0.0236 - acc: 0.5000\n",
      "Epoch 20/220\n",
      "1216/1216 [==============================] - 1s 499us/step - loss: 0.0235 - acc: 0.5000\n",
      "Epoch 21/220\n",
      "1216/1216 [==============================] - 1s 499us/step - loss: 0.0233 - acc: 0.5000\n",
      "Epoch 22/220\n",
      "1216/1216 [==============================] - 1s 501us/step - loss: 0.0231 - acc: 0.5000\n",
      "Epoch 23/220\n",
      "1216/1216 [==============================] - 1s 499us/step - loss: 0.0229 - acc: 0.5000\n",
      "Epoch 24/220\n",
      "1216/1216 [==============================] - 1s 500us/step - loss: 0.0227 - acc: 0.5000\n",
      "Epoch 25/220\n",
      "1216/1216 [==============================] - 1s 495us/step - loss: 0.0225 - acc: 0.5000\n",
      "Epoch 26/220\n",
      "1216/1216 [==============================] - 1s 493us/step - loss: 0.0223 - acc: 0.5000\n",
      "Epoch 27/220\n",
      "1216/1216 [==============================] - 1s 496us/step - loss: 0.0221 - acc: 0.5000\n",
      "Epoch 28/220\n",
      "1216/1216 [==============================] - 1s 503us/step - loss: 0.0219 - acc: 0.5000\n",
      "Epoch 29/220\n",
      "1216/1216 [==============================] - 1s 496us/step - loss: 0.0217 - acc: 0.5000\n",
      "Epoch 30/220\n",
      "1216/1216 [==============================] - 1s 502us/step - loss: 0.0215 - acc: 0.5000\n",
      "Epoch 31/220\n",
      "1216/1216 [==============================] - 1s 500us/step - loss: 0.0213 - acc: 0.5000\n",
      "Epoch 32/220\n",
      "1216/1216 [==============================] - 1s 505us/step - loss: 0.0211 - acc: 0.5000\n",
      "Epoch 33/220\n",
      "1216/1216 [==============================] - 1s 492us/step - loss: 0.0209 - acc: 0.5000\n",
      "Epoch 34/220\n",
      "1216/1216 [==============================] - 1s 504us/step - loss: 0.0207 - acc: 0.5000\n",
      "Epoch 35/220\n",
      "1216/1216 [==============================] - 1s 499us/step - loss: 0.0205 - acc: 0.5000 0s - loss: 0.0202 - \n",
      "Epoch 36/220\n",
      "1216/1216 [==============================] - 1s 489us/step - loss: 0.0203 - acc: 0.5000\n",
      "Epoch 37/220\n",
      "1216/1216 [==============================] - 1s 497us/step - loss: 0.0200 - acc: 0.5000\n",
      "Epoch 38/220\n",
      "1216/1216 [==============================] - 1s 495us/step - loss: 0.0198 - acc: 0.5000\n",
      "Epoch 39/220\n",
      "1216/1216 [==============================] - 1s 500us/step - loss: 0.0196 - acc: 0.5000\n",
      "Epoch 40/220\n",
      "1216/1216 [==============================] - 1s 499us/step - loss: 0.0193 - acc: 0.5000\n",
      "Epoch 41/220\n",
      "1216/1216 [==============================] - 1s 499us/step - loss: 0.0191 - acc: 0.5000\n",
      "Epoch 42/220\n",
      "1216/1216 [==============================] - 1s 499us/step - loss: 0.0189 - acc: 0.5000 0s - loss: 0.0200 - \n",
      "Epoch 43/220\n",
      "1216/1216 [==============================] - 1s 504us/step - loss: 0.0186 - acc: 0.5000\n",
      "Epoch 44/220\n",
      "1216/1216 [==============================] - 1s 497us/step - loss: 0.0184 - acc: 0.5000\n",
      "Epoch 45/220\n",
      "1216/1216 [==============================] - 1s 502us/step - loss: 0.0181 - acc: 0.5000\n",
      "Epoch 46/220\n",
      "1216/1216 [==============================] - 1s 495us/step - loss: 0.0179 - acc: 0.5000\n",
      "Epoch 47/220\n",
      "1216/1216 [==============================] - 1s 490us/step - loss: 0.0176 - acc: 0.5000\n",
      "Epoch 48/220\n",
      "1216/1216 [==============================] - 1s 499us/step - loss: 0.0173 - acc: 0.5000\n",
      "Epoch 49/220\n",
      "1216/1216 [==============================] - 1s 493us/step - loss: 0.0171 - acc: 0.5000\n",
      "Epoch 50/220\n",
      "1216/1216 [==============================] - 1s 492us/step - loss: 0.0168 - acc: 0.5000\n",
      "Epoch 51/220\n",
      "1216/1216 [==============================] - ETA: 0s - loss: 0.0167 - acc: 0.494 - 1s 505us/step - loss: 0.0165 - acc: 0.5000\n",
      "Epoch 52/220\n",
      "1216/1216 [==============================] - 1s 491us/step - loss: 0.0163 - acc: 0.5000\n",
      "Epoch 53/220\n",
      "1216/1216 [==============================] - 1s 495us/step - loss: 0.0160 - acc: 0.5000\n",
      "Epoch 54/220\n",
      "1216/1216 [==============================] - 1s 500us/step - loss: 0.0157 - acc: 0.5000\n",
      "Epoch 55/220\n",
      "1216/1216 [==============================] - 1s 508us/step - loss: 0.0154 - acc: 0.5000\n",
      "Epoch 56/220\n",
      "1216/1216 [==============================] - ETA: 0s - loss: 0.0152 - acc: 0.496 - 1s 502us/step - loss: 0.0151 - acc: 0.5000\n",
      "Epoch 57/220\n",
      "1216/1216 [==============================] - 1s 501us/step - loss: 0.0149 - acc: 0.5000\n",
      "Epoch 58/220\n",
      "1216/1216 [==============================] - 1s 504us/step - loss: 0.0146 - acc: 0.5000\n",
      "Epoch 59/220\n",
      "1216/1216 [==============================] - 1s 507us/step - loss: 0.0143 - acc: 0.5000\n",
      "Epoch 60/220\n",
      "1216/1216 [==============================] - 1s 507us/step - loss: 0.0140 - acc: 0.5000 0s - loss: 0.0138 - a\n",
      "Epoch 61/220\n",
      "1216/1216 [==============================] - 1s 504us/step - loss: 0.0137 - acc: 0.5000\n",
      "Epoch 62/220\n",
      "1216/1216 [==============================] - 1s 501us/step - loss: 0.0135 - acc: 0.5000\n",
      "Epoch 63/220\n",
      "1216/1216 [==============================] - 1s 509us/step - loss: 0.0132 - acc: 0.5000\n",
      "Epoch 64/220\n",
      "1216/1216 [==============================] - 1s 499us/step - loss: 0.0129 - acc: 0.5000\n",
      "Epoch 65/220\n",
      "1216/1216 [==============================] - 1s 500us/step - loss: 0.0127 - acc: 0.5000\n",
      "Epoch 66/220\n",
      "1216/1216 [==============================] - 1s 507us/step - loss: 0.0124 - acc: 0.5000 0s - loss: 0.0128 - \n",
      "Epoch 67/220\n",
      "1216/1216 [==============================] - 1s 500us/step - loss: 0.0122 - acc: 0.5000 0s - loss: 0.0132 - \n",
      "Epoch 68/220\n",
      "1216/1216 [==============================] - 1s 511us/step - loss: 0.0119 - acc: 0.5000\n",
      "Epoch 69/220\n",
      "1216/1216 [==============================] - ETA: 0s - loss: 0.0116 - acc: 0.506 - 1s 500us/step - loss: 0.0116 - acc: 0.5016\n",
      "Epoch 70/220\n",
      "1216/1216 [==============================] - 1s 506us/step - loss: 0.0114 - acc: 0.5025\n",
      "Epoch 71/220\n",
      "1216/1216 [==============================] - 1s 504us/step - loss: 0.0112 - acc: 0.5033\n",
      "Epoch 72/220\n",
      "1216/1216 [==============================] - 1s 500us/step - loss: 0.0109 - acc: 0.5066\n",
      "Epoch 73/220\n",
      "1216/1216 [==============================] - 1s 509us/step - loss: 0.0107 - acc: 0.5123\n",
      "Epoch 74/220\n",
      "1216/1216 [==============================] - 1s 499us/step - loss: 0.0105 - acc: 0.5156\n",
      "Epoch 75/220\n",
      "1216/1216 [==============================] - 1s 510us/step - loss: 0.0102 - acc: 0.5214\n",
      "Epoch 76/220\n",
      "1216/1216 [==============================] - 1s 503us/step - loss: 0.0100 - acc: 0.5271\n",
      "Epoch 77/220\n",
      "1216/1216 [==============================] - 1s 500us/step - loss: 0.0098 - acc: 0.5378\n",
      "Epoch 78/220\n",
      "1216/1216 [==============================] - 1s 505us/step - loss: 0.0096 - acc: 0.5493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/220\n",
      "1216/1216 [==============================] - 1s 490us/step - loss: 0.0094 - acc: 0.5600\n",
      "Epoch 80/220\n",
      "1216/1216 [==============================] - 1s 491us/step - loss: 0.0092 - acc: 0.5707 0s - loss: 0.0090 - \n",
      "Epoch 81/220\n",
      "1216/1216 [==============================] - 1s 490us/step - loss: 0.0090 - acc: 0.5740\n",
      "Epoch 82/220\n",
      "1216/1216 [==============================] - 1s 486us/step - loss: 0.0088 - acc: 0.5863\n",
      "Epoch 83/220\n",
      "1216/1216 [==============================] - 1s 487us/step - loss: 0.0087 - acc: 0.5987\n",
      "Epoch 84/220\n",
      "1216/1216 [==============================] - 1s 504us/step - loss: 0.0085 - acc: 0.6127\n",
      "Epoch 85/220\n",
      "1216/1216 [==============================] - 1s 471us/step - loss: 0.0083 - acc: 0.6308\n",
      "Epoch 86/220\n",
      "1216/1216 [==============================] - 1s 467us/step - loss: 0.0081 - acc: 0.6398\n",
      "Epoch 87/220\n",
      "1216/1216 [==============================] - 1s 477us/step - loss: 0.0080 - acc: 0.6571\n",
      "Epoch 88/220\n",
      "1216/1216 [==============================] - 1s 478us/step - loss: 0.0078 - acc: 0.6678\n",
      "Epoch 89/220\n",
      "1216/1216 [==============================] - 1s 487us/step - loss: 0.0076 - acc: 0.6826\n",
      "Epoch 90/220\n",
      "1216/1216 [==============================] - 1s 484us/step - loss: 0.0075 - acc: 0.6974\n",
      "Epoch 91/220\n",
      "1216/1216 [==============================] - 1s 486us/step - loss: 0.0073 - acc: 0.7089\n",
      "Epoch 92/220\n",
      "1216/1216 [==============================] - 1s 485us/step - loss: 0.0072 - acc: 0.7212\n",
      "Epoch 93/220\n",
      "1216/1216 [==============================] - 1s 484us/step - loss: 0.0070 - acc: 0.7311\n",
      "Epoch 94/220\n",
      "1216/1216 [==============================] - 1s 487us/step - loss: 0.0069 - acc: 0.7442\n",
      "Epoch 95/220\n",
      "1216/1216 [==============================] - 1s 485us/step - loss: 0.0068 - acc: 0.7484\n",
      "Epoch 96/220\n",
      "1216/1216 [==============================] - 1s 496us/step - loss: 0.0066 - acc: 0.7623\n",
      "Epoch 97/220\n",
      "1216/1216 [==============================] - 1s 484us/step - loss: 0.0065 - acc: 0.7722\n",
      "Epoch 98/220\n",
      "1216/1216 [==============================] - 1s 483us/step - loss: 0.0064 - acc: 0.7796\n",
      "Epoch 99/220\n",
      "1216/1216 [==============================] - 1s 486us/step - loss: 0.0062 - acc: 0.7887\n",
      "Epoch 100/220\n",
      "1216/1216 [==============================] - 1s 490us/step - loss: 0.0061 - acc: 0.7985\n",
      "Epoch 101/220\n",
      "1216/1216 [==============================] - 1s 498us/step - loss: 0.0060 - acc: 0.8043\n",
      "Epoch 102/220\n",
      "1216/1216 [==============================] - 1s 484us/step - loss: 0.0059 - acc: 0.8067\n",
      "Epoch 103/220\n",
      "1216/1216 [==============================] - 1s 490us/step - loss: 0.0057 - acc: 0.8109\n",
      "Epoch 104/220\n",
      "1216/1216 [==============================] - 1s 488us/step - loss: 0.0056 - acc: 0.8125\n",
      "Epoch 105/220\n",
      "1216/1216 [==============================] - 1s 487us/step - loss: 0.0055 - acc: 0.8183\n",
      "Epoch 106/220\n",
      "1216/1216 [==============================] - 1s 483us/step - loss: 0.0054 - acc: 0.8232\n",
      "Epoch 107/220\n",
      "1216/1216 [==============================] - 1s 489us/step - loss: 0.0053 - acc: 0.8281\n",
      "Epoch 108/220\n",
      "1216/1216 [==============================] - 1s 490us/step - loss: 0.0052 - acc: 0.8314\n",
      "Epoch 109/220\n",
      "1216/1216 [==============================] - 1s 486us/step - loss: 0.0051 - acc: 0.8396\n",
      "Epoch 110/220\n",
      "1216/1216 [==============================] - 1s 489us/step - loss: 0.0050 - acc: 0.8405\n",
      "Epoch 111/220\n",
      "1216/1216 [==============================] - 1s 489us/step - loss: 0.0049 - acc: 0.8503\n",
      "Epoch 112/220\n",
      "1216/1216 [==============================] - 1s 487us/step - loss: 0.0048 - acc: 0.8553\n",
      "Epoch 113/220\n",
      "1216/1216 [==============================] - 1s 493us/step - loss: 0.0047 - acc: 0.8643\n",
      "Epoch 114/220\n",
      "1216/1216 [==============================] - 1s 487us/step - loss: 0.0046 - acc: 0.8643\n",
      "Epoch 115/220\n",
      "1216/1216 [==============================] - 1s 497us/step - loss: 0.0045 - acc: 0.8709\n",
      "Epoch 116/220\n",
      "1216/1216 [==============================] - 1s 490us/step - loss: 0.0044 - acc: 0.8742\n",
      "Epoch 117/220\n",
      "1216/1216 [==============================] - 1s 488us/step - loss: 0.0043 - acc: 0.8783\n",
      "Epoch 118/220\n",
      "1216/1216 [==============================] - 1s 495us/step - loss: 0.0042 - acc: 0.8808\n",
      "Epoch 119/220\n",
      "1216/1216 [==============================] - 1s 512us/step - loss: 0.0041 - acc: 0.8832\n",
      "Epoch 120/220\n",
      "1216/1216 [==============================] - 1s 521us/step - loss: 0.0041 - acc: 0.8849\n",
      "Epoch 121/220\n",
      "1216/1216 [==============================] - 1s 505us/step - loss: 0.0040 - acc: 0.8882\n",
      "Epoch 122/220\n",
      "1216/1216 [==============================] - 1s 486us/step - loss: 0.0039 - acc: 0.8923\n",
      "Epoch 123/220\n",
      "1216/1216 [==============================] - 1s 506us/step - loss: 0.0038 - acc: 0.8964\n",
      "Epoch 124/220\n",
      "1216/1216 [==============================] - 1s 486us/step - loss: 0.0037 - acc: 0.8964 0s - loss: 0.0042 - a\n",
      "Epoch 125/220\n",
      "1216/1216 [==============================] - 1s 487us/step - loss: 0.0037 - acc: 0.9038\n",
      "Epoch 126/220\n",
      "1216/1216 [==============================] - 1s 489us/step - loss: 0.0036 - acc: 0.9054\n",
      "Epoch 127/220\n",
      "1216/1216 [==============================] - 1s 486us/step - loss: 0.0035 - acc: 0.9112\n",
      "Epoch 128/220\n",
      "1216/1216 [==============================] - 1s 484us/step - loss: 0.0035 - acc: 0.9137\n",
      "Epoch 129/220\n",
      "1216/1216 [==============================] - 1s 490us/step - loss: 0.0034 - acc: 0.9169\n",
      "Epoch 130/220\n",
      "1216/1216 [==============================] - 1s 483us/step - loss: 0.0033 - acc: 0.9178\n",
      "Epoch 131/220\n",
      "1216/1216 [==============================] - 1s 510us/step - loss: 0.0033 - acc: 0.9186\n",
      "Epoch 132/220\n",
      "1216/1216 [==============================] - 1s 483us/step - loss: 0.0032 - acc: 0.9194\n",
      "Epoch 133/220\n",
      "1216/1216 [==============================] - 1s 495us/step - loss: 0.0031 - acc: 0.9235\n",
      "Epoch 134/220\n",
      "1216/1216 [==============================] - 1s 490us/step - loss: 0.0031 - acc: 0.9243\n",
      "Epoch 135/220\n",
      "1216/1216 [==============================] - 1s 491us/step - loss: 0.0030 - acc: 0.9260\n",
      "Epoch 136/220\n",
      "1216/1216 [==============================] - 1s 495us/step - loss: 0.0030 - acc: 0.9260\n",
      "Epoch 137/220\n",
      "1216/1216 [==============================] - 1s 488us/step - loss: 0.0029 - acc: 0.9276\n",
      "Epoch 138/220\n",
      "1216/1216 [==============================] - 1s 491us/step - loss: 0.0029 - acc: 0.9309\n",
      "Epoch 139/220\n",
      "1216/1216 [==============================] - 1s 524us/step - loss: 0.0028 - acc: 0.9326\n",
      "Epoch 140/220\n",
      "1216/1216 [==============================] - 1s 482us/step - loss: 0.0028 - acc: 0.9326\n",
      "Epoch 141/220\n",
      "1216/1216 [==============================] - 1s 495us/step - loss: 0.0027 - acc: 0.9350\n",
      "Epoch 142/220\n",
      "1216/1216 [==============================] - 1s 499us/step - loss: 0.0027 - acc: 0.9359\n",
      "Epoch 143/220\n",
      "1216/1216 [==============================] - 1s 488us/step - loss: 0.0026 - acc: 0.9375\n",
      "Epoch 144/220\n",
      "1216/1216 [==============================] - 1s 493us/step - loss: 0.0026 - acc: 0.9400\n",
      "Epoch 145/220\n",
      "1216/1216 [==============================] - 1s 502us/step - loss: 0.0025 - acc: 0.9424\n",
      "Epoch 146/220\n",
      "1216/1216 [==============================] - 1s 493us/step - loss: 0.0025 - acc: 0.9449 0s - loss: 0.0026 - \n",
      "Epoch 147/220\n",
      "1216/1216 [==============================] - 1s 490us/step - loss: 0.0024 - acc: 0.9449\n",
      "Epoch 148/220\n",
      "1216/1216 [==============================] - 1s 496us/step - loss: 0.0024 - acc: 0.9465\n",
      "Epoch 149/220\n",
      "1216/1216 [==============================] - 1s 490us/step - loss: 0.0023 - acc: 0.9474\n",
      "Epoch 150/220\n",
      "1216/1216 [==============================] - 1s 490us/step - loss: 0.0023 - acc: 0.9482\n",
      "Epoch 151/220\n",
      "1216/1216 [==============================] - 1s 495us/step - loss: 0.0023 - acc: 0.9490\n",
      "Epoch 152/220\n",
      "1216/1216 [==============================] - 1s 488us/step - loss: 0.0022 - acc: 0.9498\n",
      "Epoch 153/220\n",
      "1216/1216 [==============================] - 1s 495us/step - loss: 0.0022 - acc: 0.9498\n",
      "Epoch 154/220\n",
      "1216/1216 [==============================] - 1s 495us/step - loss: 0.0021 - acc: 0.9507\n",
      "Epoch 155/220\n",
      "1216/1216 [==============================] - 1s 489us/step - loss: 0.0021 - acc: 0.9515\n",
      "Epoch 156/220\n",
      "1216/1216 [==============================] - 1s 490us/step - loss: 0.0021 - acc: 0.9523\n",
      "Epoch 157/220\n",
      "1216/1216 [==============================] - 1s 490us/step - loss: 0.0020 - acc: 0.9539\n",
      "Epoch 158/220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1216/1216 [==============================] - 1s 484us/step - loss: 0.0020 - acc: 0.9556\n",
      "Epoch 159/220\n",
      "1216/1216 [==============================] - 1s 495us/step - loss: 0.0020 - acc: 0.9572\n",
      "Epoch 160/220\n",
      "1216/1216 [==============================] - 1s 515us/step - loss: 0.0019 - acc: 0.9589\n",
      "Epoch 161/220\n",
      "1216/1216 [==============================] - 1s 519us/step - loss: 0.0019 - acc: 0.9597\n",
      "Epoch 162/220\n",
      "1216/1216 [==============================] - 1s 494us/step - loss: 0.0019 - acc: 0.9597\n",
      "Epoch 163/220\n",
      "1216/1216 [==============================] - 1s 483us/step - loss: 0.0019 - acc: 0.9597\n",
      "Epoch 164/220\n",
      "1216/1216 [==============================] - 1s 500us/step - loss: 0.0018 - acc: 0.9597\n",
      "Epoch 165/220\n",
      "1216/1216 [==============================] - 1s 529us/step - loss: 0.0018 - acc: 0.9605\n",
      "Epoch 166/220\n",
      "1216/1216 [==============================] - 1s 522us/step - loss: 0.0018 - acc: 0.9613\n",
      "Epoch 167/220\n",
      "1216/1216 [==============================] - 1s 516us/step - loss: 0.0017 - acc: 0.9630\n",
      "Epoch 168/220\n",
      "1216/1216 [==============================] - 1s 508us/step - loss: 0.0017 - acc: 0.9638\n",
      "Epoch 169/220\n",
      "1216/1216 [==============================] - 1s 522us/step - loss: 0.0017 - acc: 0.9646\n",
      "Epoch 170/220\n",
      "1216/1216 [==============================] - 1s 513us/step - loss: 0.0017 - acc: 0.9655\n",
      "Epoch 171/220\n",
      "1216/1216 [==============================] - 1s 483us/step - loss: 0.0016 - acc: 0.9655\n",
      "Epoch 172/220\n",
      "1216/1216 [==============================] - 1s 487us/step - loss: 0.0016 - acc: 0.9663\n",
      "Epoch 173/220\n",
      "1216/1216 [==============================] - 1s 470us/step - loss: 0.0016 - acc: 0.9663\n",
      "Epoch 174/220\n",
      "1216/1216 [==============================] - 1s 504us/step - loss: 0.0016 - acc: 0.9663\n",
      "Epoch 175/220\n",
      "1216/1216 [==============================] - 1s 488us/step - loss: 0.0015 - acc: 0.9671\n",
      "Epoch 176/220\n",
      "1216/1216 [==============================] - 1s 489us/step - loss: 0.0015 - acc: 0.9671\n",
      "Epoch 177/220\n",
      "1216/1216 [==============================] - 1s 490us/step - loss: 0.0015 - acc: 0.9687\n",
      "Epoch 178/220\n",
      "1216/1216 [==============================] - 1s 495us/step - loss: 0.0015 - acc: 0.9696\n",
      "Epoch 179/220\n",
      "1216/1216 [==============================] - 1s 490us/step - loss: 0.0015 - acc: 0.9704ETA: 0s - loss: 0.0016 - a\n",
      "Epoch 180/220\n",
      "1216/1216 [==============================] - 1s 485us/step - loss: 0.0014 - acc: 0.9712\n",
      "Epoch 181/220\n",
      "1216/1216 [==============================] - 1s 498us/step - loss: 0.0014 - acc: 0.9720\n",
      "Epoch 182/220\n",
      "1216/1216 [==============================] - 1s 483us/step - loss: 0.0014 - acc: 0.9720\n",
      "Epoch 183/220\n",
      "1216/1216 [==============================] - 1s 489us/step - loss: 0.0014 - acc: 0.9720\n",
      "Epoch 184/220\n",
      "1216/1216 [==============================] - 1s 486us/step - loss: 0.0014 - acc: 0.9729\n",
      "Epoch 185/220\n",
      "1216/1216 [==============================] - 1s 490us/step - loss: 0.0013 - acc: 0.9729\n",
      "Epoch 186/220\n",
      "1216/1216 [==============================] - 1s 488us/step - loss: 0.0013 - acc: 0.9745\n",
      "Epoch 187/220\n",
      "1216/1216 [==============================] - 1s 486us/step - loss: 0.0013 - acc: 0.9762\n",
      "Epoch 188/220\n",
      "1216/1216 [==============================] - 1s 488us/step - loss: 0.0013 - acc: 0.9770\n",
      "Epoch 189/220\n",
      "1216/1216 [==============================] - 1s 497us/step - loss: 0.0013 - acc: 0.9770\n",
      "Epoch 190/220\n",
      "1216/1216 [==============================] - 1s 493us/step - loss: 0.0013 - acc: 0.9778\n",
      "Epoch 191/220\n",
      "1216/1216 [==============================] - 1s 495us/step - loss: 0.0012 - acc: 0.9778\n",
      "Epoch 192/220\n",
      "1216/1216 [==============================] - 1s 487us/step - loss: 0.0012 - acc: 0.9778\n",
      "Epoch 193/220\n",
      "1216/1216 [==============================] - 1s 491us/step - loss: 0.0012 - acc: 0.9778\n",
      "Epoch 194/220\n",
      "1216/1216 [==============================] - 1s 489us/step - loss: 0.0012 - acc: 0.9778\n",
      "Epoch 195/220\n",
      "1216/1216 [==============================] - 1s 491us/step - loss: 0.0012 - acc: 0.9778\n",
      "Epoch 196/220\n",
      "1216/1216 [==============================] - 1s 488us/step - loss: 0.0012 - acc: 0.9778\n",
      "Epoch 197/220\n",
      "1216/1216 [==============================] - 1s 490us/step - loss: 0.0012 - acc: 0.9778\n",
      "Epoch 198/220\n",
      "1216/1216 [==============================] - 1s 491us/step - loss: 0.0011 - acc: 0.9803\n",
      "Epoch 199/220\n",
      "1216/1216 [==============================] - 1s 486us/step - loss: 0.0011 - acc: 0.9811\n",
      "Epoch 200/220\n",
      "1216/1216 [==============================] - 1s 488us/step - loss: 0.0011 - acc: 0.9819\n",
      "Epoch 201/220\n",
      "1216/1216 [==============================] - 1s 492us/step - loss: 0.0011 - acc: 0.9819\n",
      "Epoch 202/220\n",
      "1216/1216 [==============================] - 1s 492us/step - loss: 0.0011 - acc: 0.9819\n",
      "Epoch 203/220\n",
      "1216/1216 [==============================] - 1s 490us/step - loss: 0.0011 - acc: 0.9819\n",
      "Epoch 204/220\n",
      "1216/1216 [==============================] - 1s 458us/step - loss: 0.0011 - acc: 0.9819\n",
      "Epoch 205/220\n",
      "1216/1216 [==============================] - 1s 495us/step - loss: 0.0010 - acc: 0.9819\n",
      "Epoch 206/220\n",
      "1216/1216 [==============================] - 1s 489us/step - loss: 0.0010 - acc: 0.9819\n",
      "Epoch 207/220\n",
      "1216/1216 [==============================] - 1s 495us/step - loss: 0.0010 - acc: 0.9836\n",
      "Epoch 208/220\n",
      "1216/1216 [==============================] - 1s 505us/step - loss: 0.0010 - acc: 0.9836\n",
      "Epoch 209/220\n",
      "1216/1216 [==============================] - 1s 485us/step - loss: 0.0010 - acc: 0.9836 0s - loss: 0.0019 - \n",
      "Epoch 210/220\n",
      "1216/1216 [==============================] - 1s 486us/step - loss: 9.9070e-04 - acc: 0.9836\n",
      "Epoch 211/220\n",
      "1216/1216 [==============================] - 1s 490us/step - loss: 9.7981e-04 - acc: 0.9844\n",
      "Epoch 212/220\n",
      "1216/1216 [==============================] - 1s 482us/step - loss: 9.6885e-04 - acc: 0.9836\n",
      "Epoch 213/220\n",
      "1216/1216 [==============================] - 1s 490us/step - loss: 9.5819e-04 - acc: 0.9844\n",
      "Epoch 214/220\n",
      "1216/1216 [==============================] - 1s 483us/step - loss: 9.4768e-04 - acc: 0.9844\n",
      "Epoch 215/220\n",
      "1216/1216 [==============================] - 1s 486us/step - loss: 9.3750e-04 - acc: 0.9860\n",
      "Epoch 216/220\n",
      "1216/1216 [==============================] - 1s 487us/step - loss: 9.2741e-04 - acc: 0.9860\n",
      "Epoch 217/220\n",
      "1216/1216 [==============================] - 1s 486us/step - loss: 9.1745e-04 - acc: 0.9860\n",
      "Epoch 218/220\n",
      "1216/1216 [==============================] - 1s 490us/step - loss: 9.0769e-04 - acc: 0.9860\n",
      "Epoch 219/220\n",
      "1216/1216 [==============================] - 1s 492us/step - loss: 8.9793e-04 - acc: 0.98600s - loss: 9.3364e-04 - ac\n",
      "Epoch 220/220\n",
      "1216/1216 [==============================] - 1s 489us/step - loss: 8.8860e-04 - acc: 0.9860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22afd199b00>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight={0  :0.5,   1 : 1e-2}\n",
    "estimator = KerasClassifier(build_fn=create_deep_learning_model,epochs=220, batch_size=10, class_weight =class_weight)\n",
    "estimator.fit(X_train, Y_train,class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(Y_test, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1   5]\n",
      " [ 15 139]]\n",
      "recall = 0.16666666666666666\n",
      "specificity = 0.9025974025974026\n",
      "acc = 0.875\n"
     ]
    }
   ],
   "source": [
    "print(conf_mat)\n",
    "recall = conf_mat[0][0]/sum(conf_mat[0])\n",
    "specificity = conf_mat[1][1] / sum(conf_mat[1])\n",
    "acc = (conf_mat[0][0] + conf_mat[1][1]) / sum(sum(conf_mat))\n",
    "print(\"recall = \" + str(recall))\n",
    "print(\"specificity = \" + str(specificity))\n",
    "print(\"acc = \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.2967 - acc: 0.9575\n",
      "Epoch 2/5\n",
      "636/636 [==============================] - 1s 1ms/step - loss: 0.0563 - acc: 0.9654\n",
      "Epoch 3/5\n",
      "636/636 [==============================] - 1s 1ms/step - loss: 0.0201 - acc: 0.9780\n",
      "Epoch 4/5\n",
      "636/636 [==============================] - 1s 1ms/step - loss: 0.0082 - acc: 0.9937- ETA: 0s - loss: 0.0179 -\n",
      "Epoch 5/5\n",
      "636/636 [==============================] - 1s 1ms/step - loss: 0.0013 - acc: 0.9984\n",
      "Epoch 1/5\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.1920 - acc: 0.9513\n",
      "Epoch 2/5\n",
      "636/636 [==============================] - 1s 1ms/step - loss: 0.0802 - acc: 0.9481\n",
      "Epoch 3/5\n",
      "636/636 [==============================] - 1s 1ms/step - loss: 0.0338 - acc: 0.9701\n",
      "Epoch 4/5\n",
      "636/636 [==============================] - 1s 1ms/step - loss: 0.0074 - acc: 0.9953\n",
      "Epoch 5/5\n",
      "636/636 [==============================] - 1s 1ms/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 1/5\n",
      "637/637 [==============================] - 1s 2ms/step - loss: 0.2149 - acc: 0.9074\n",
      "Epoch 2/5\n",
      "637/637 [==============================] - 1s 1ms/step - loss: 0.0665 - acc: 0.9670\n",
      "Epoch 3/5\n",
      "637/637 [==============================] - 1s 1ms/step - loss: 0.0326 - acc: 0.9780\n",
      "Epoch 4/5\n",
      "637/637 [==============================] - 1s 1ms/step - loss: 0.0220 - acc: 0.9859\n",
      "Epoch 5/5\n",
      "637/637 [==============================] - 1s 1ms/step - loss: 0.0052 - acc: 0.9969\n",
      "Epoch 1/5\n",
      "637/637 [==============================] - 1s 2ms/step - loss: 0.1875 - acc: 0.9513\n",
      "Epoch 2/5\n",
      "637/637 [==============================] - 1s 1ms/step - loss: 0.0589 - acc: 0.9702\n",
      "Epoch 3/5\n",
      "637/637 [==============================] - 1s 1ms/step - loss: 0.0252 - acc: 0.9843\n",
      "Epoch 4/5\n",
      "637/637 [==============================] - 1s 1ms/step - loss: 0.0041 - acc: 0.9969\n",
      "Epoch 5/5\n",
      "637/637 [==============================] - 1s 1ms/step - loss: 3.3811e-05 - acc: 1.0000\n",
      "Epoch 1/5\n",
      "638/638 [==============================] - 1s 2ms/step - loss: 0.2922 - acc: 0.9436\n",
      "Epoch 2/5\n",
      "638/638 [==============================] - 1s 1ms/step - loss: 0.0935 - acc: 0.9608\n",
      "Epoch 3/5\n",
      "638/638 [==============================] - 1s 1ms/step - loss: 0.0300 - acc: 0.9906\n",
      "Epoch 4/5\n",
      "638/638 [==============================] - 1s 1ms/step - loss: 0.0038 - acc: 0.9969- ETA: 0s - loss: 0.0044 \n",
      "Epoch 5/5\n",
      "638/638 [==============================] - 1s 1ms/step - loss: 0.0218 - acc: 0.9969\n",
      "[[  2  32]\n",
      " [ 19 743]]\n",
      "recall = 0.058823529411764705\n",
      "specificity = 0.9750656167979003\n",
      "acc = 0.9359296482412061\n"
     ]
    }
   ],
   "source": [
    "estimator = KerasClassifier(build_fn=create_deep_learning_model,epochs=5,batch_size=5)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "y_pred = cross_val_predict(estimator, X,Y,cv=kfold)\n",
    "conf_mat = confusion_matrix(Y, y_pred)\n",
    "print(conf_mat)\n",
    "recall = conf_mat[0][0]/sum(conf_mat[0])\n",
    "specificity = conf_mat[1][1] / sum(conf_mat[1])\n",
    "acc = (conf_mat[0][0] + conf_mat[1][1]) / sum(sum(conf_mat))\n",
    "print(\"recall = \" + str(recall))\n",
    "print(\"specificity = \" + str(specificity))\n",
    "print(\"acc = \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
